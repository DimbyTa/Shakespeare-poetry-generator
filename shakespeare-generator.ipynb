{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport string\nfrom collections import Counter\nfrom argparse import Namespace\nimport json\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-23T06:36:09.793346Z","iopub.execute_input":"2023-08-23T06:36:09.793768Z","iopub.status.idle":"2023-08-23T06:36:13.668280Z","shell.execute_reply.started":"2023-08-23T06:36:09.793729Z","shell.execute_reply":"2023-08-23T06:36:13.666264Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/splitted-poems/famous_poems_split.csv\n/kaggle/input/splitted-poems/surnames_with_splits.csv\n/kaggle/input/splitted-poems/famous_poems_cleaned_single_string_splitted.csv\n/kaggle/input/cleaning-poems/__results__.html\n/kaggle/input/cleaning-poems/famous_poems_cleaned_single_string.csv\n/kaggle/input/cleaning-poems/famous_poems_cleaned.csv\n/kaggle/input/cleaning-poems/__notebook__.ipynb\n/kaggle/input/cleaning-poems/__output__.json\n/kaggle/input/cleaning-poems/custom.css\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This work is inspired by inspired by Natural Language Processing in Pytorch by Delip Rao and Brian McMahan.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\npoems_df = pd.read_csv('/kaggle/input/cleaning-poems/famous_poems_cleaned_single_string.csv')\n\nlength = []\nfor poem in poems_df['poem_body']:\n    length.append(len(poem))\nlength = np.array(length)\nprint(\"max \", np.max(length))\nprint(\"min \", np.min(length))\nprint(\"mean \", np.mean(length))\nprint(\"median \", np.median(length))\nprint(\"std \", np.std(length))\nplt.hist(length)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.671712Z","iopub.execute_input":"2023-08-23T06:36:13.672429Z","iopub.status.idle":"2023-08-23T06:36:13.908253Z","shell.execute_reply.started":"2023-08-23T06:36:13.672392Z","shell.execute_reply":"2023-08-23T06:36:13.906764Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"max  14287\nmin  222\nmean  1040.0833333333333\nmedian  595.5\nstd  2089.359160457792\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We see that the maximum length of character is 14287. Using that maximum sequence length as in the book of Delip Rao and Brian McMahan will make us run into vanishing gradient. Also, most of the poems in the dataset have a length between 222 and 2000 as we can see on the plot. It is better to subdivide the sequences into smaller sequences so we do not run into vanishing gradient, and we will also keep the number of padding sequence to the minimum.","metadata":{}},{"cell_type":"markdown","source":"## Creating the vocabulary\n    It consists of all the unique tokens (here characters) in the dataset and their respective count.","metadata":{}},{"cell_type":"code","source":"class Vocabulary:\n    \"\"\"\n    class to process text and extract Vocabulary (tokens in the text) \n    for mapping (tokens => integer)\n    \"\"\"\n    def __init__(self, token_to_idx = None, add_unk = True, unk_token = '<UNK>'):\n        \"\"\"\n        Args:\n            token_to_idx: a dict mapping tokens to indices, if you already have a particular mapping\n            add_unk: a boolean indicating to add or not the unk_token '<UNK>'\n            unk_token: the token used to represent and process unknown token, what we will add into the vocabulary\n        \"\"\"\n        if token_to_idx is None:\n            token_to_idx = {}\n        self._token_to_idx = token_to_idx\n        self._idx_to_token = {idx: token for token,idx in self._token_to_idx.items()}\n        self._add_unk = add_unk\n        self._unk_token = unk_token\n        self.unk_index = -1\n        \n        if add_unk:\n            self.unk_index = self.add_token(unk_token)\n            \n    def to_serializable(self):\n        \"\"\"\n        returns a dictionary that can be serialized\n        \"\"\"\n        return {'token_to_idx': self._token_to_idx, 'add_unk': self._add_unk,\n                   'unk_token': self._unk_token}\n    \n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\"\n        instantiates the Vocabulary from a serialized dictionary\n        \"\"\"\n        return cls(**contents)\n    \n    def add_token(self, token):\n        \"\"\"\n        Updates mapping dict based on token\n        \n        Args:\n            token (str): the item to be added to the Vocabulary\n        Returns:\n            index (int): the integer corresponding to the token in the mapping\n        \"\"\"\n        if token in self._token_to_idx:\n            index = self._token_to_idx[token]\n        else:\n            index = len(self._token_to_idx)\n            self._token_to_idx[token] = index\n            self._idx_to_token[index] = token\n        return index\n    \n    def add_many(self, tokens):\n        \"\"\"Add a list of tokens into the Vocabulary\n        \n        Args:\n            tokens (list): a list of string tokens\n        Returns:\n            indices (list): a list of indices corresponding to the tokens\n        \"\"\"\n        return [self.add_token(token) for token in tokens]\n    \n    def lookup_token(self, token):\n        \"\"\"\n        Retrieves the index of token in the mapping, or return the index of\n        UNK if it isn't present\n        Args:\n            token (str): the token to look up for\n        Returns:\n        index (str): the index corresponding to the token\n        Notes:\n            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n              for the UNK functionality\n        \"\"\"\n        if self.add_unk:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]\n        \n    def lookup_index(self,index):\n        \"\"\"\n        Returns the token associated with index\n        Args: \n            index (int): the index to look up for\n        Returns:\n            token (str): the token associated with index\n        Raises:\n            KeyError if index is not in the Vocabulary\n        \"\"\"\n        if index not in self._idx_to_token:\n            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n        return self._idx_to_token[index]\n    \n    def __str__(self):\n        return \"<Vocabulary(size=%d)>\" % len(self)\n    def __len__(self):\n        return len(self._token_to_idx)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.909822Z","iopub.execute_input":"2023-08-23T06:36:13.910284Z","iopub.status.idle":"2023-08-23T06:36:13.923859Z","shell.execute_reply.started":"2023-08-23T06:36:13.910248Z","shell.execute_reply":"2023-08-23T06:36:13.922081Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class SequenceVocabulary(Vocabulary):\n    def __init__(self, token_to_idx=None, add_unk = True,unk_token=\"<UNK>\",\n                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n                 end_seq_token=\"<END>\"):\n\n        super(SequenceVocabulary, self).__init__(token_to_idx)\n\n        self._mask_token = mask_token\n        self._unk_token = unk_token\n        self._begin_seq_token = begin_seq_token\n        self._end_seq_token = end_seq_token\n\n        self.mask_index = self.add_token(self._mask_token)\n        self.unk_index = self.add_token(self._unk_token)\n        self.begin_seq_index = self.add_token(self._begin_seq_token)\n        self.end_seq_index = self.add_token(self._end_seq_token)\n\n    def to_serializable(self):\n        contents = super(SequenceVocabulary, self).to_serializable()\n        contents.update({'unk_token': self._unk_token,\n                         'mask_token': self._mask_token,\n                         'begin_seq_token': self._begin_seq_token,\n                         'end_seq_token': self._end_seq_token})\n        return contents\n\n    def lookup_token(self, token):\n        \"\"\"Retrieve the index associated with the token \n          or the UNK index if token isn't present.\n        \n        Args:\n            token (str): the token to look up \n        Returns:\n            index (int): the index corresponding to the token\n        Notes:\n            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n              for the UNK functionality \n        \"\"\"\n        #print(token)\n        #print(type(token))\n        if self.unk_index >= 0:\n            return self._token_to_idx.get(token, self.unk_index)\n        else:\n            return self._token_to_idx[token]","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.928490Z","iopub.execute_input":"2023-08-23T06:36:13.929448Z","iopub.status.idle":"2023-08-23T06:36:13.946932Z","shell.execute_reply.started":"2023-08-23T06:36:13.929412Z","shell.execute_reply":"2023-08-23T06:36:13.945372Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Vectorizer\nThe Vectorizer is in charge of transforming the text dataset into vectors to process. \nSince we are training to generate poems, we want the model to predict the sequence of\ncharacters we are training him on, that is why we split the vectors into two lists vectors: \n* from_vectors; the observations;\n* to_vectors, the targets. \nThey are vectors of integers given by the mapping of SequenceVocabulary. \n\nHere, we subdivide the input sequence into subsequences of length subsequence_length and group them into the lists from_vectors and to_vectors. We do this to prevent vanishing gradient from sequences that are too long. ","metadata":{}},{"cell_type":"code","source":"class PoemVectorizer(object):\n    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n    def __init__(self, char_vocab):\n        \"\"\"\n        Args:\n            char_vocab (Vocabulary): maps words to integers\n            \n        \"\"\"\n        self.char_vocab = char_vocab\n        \n    def vectorize(self, poem, subsequence_length=64, stride=1):\n        \"\"\"Vectorize a poem into a vector of observations and targets\n        \n        The outputs are the vectorized poem split into subsequences:\n            poem[:-1] and poem[1:]\n        At each timestep, the first vector is the observation and the second vector is the target. \n        \n        Args:\n            poem (str): the poem to be vectorized\n            subsequence_length (int): length of each subsequence\n            stride (int): stride for overlapping subsequences\n        Returns:\n            a tuple of lists: (from_vectors, to_vectors)\n            from_vectors (list of numpy.ndarray): list of observation vectors \n            to_vectors (list of numpy.ndarray): list of target prediction vectors\n        \"\"\"\n        indices = [self.char_vocab.begin_seq_index] \n        indices.extend(self.char_vocab.lookup_token(token) for token in poem)\n        indices.append(self.char_vocab.end_seq_index)\n        len_indices = len(indices)\n        \n        if subsequence_length < 0:\n            subsequence_length = len_indices - 1\n\n        num_subsequences = (len_indices - 1) // subsequence_length\n        total_length = num_subsequences * subsequence_length\n        \n        from_indices = [indices[i:i+subsequence_length] for i in range(0, total_length, stride)]\n        to_indices = [indices[i+1:i+subsequence_length+1] for i in range(0, total_length, stride)]\n        # Pad subsequences to have the same length\n        max_length = max(len(subseq) for subseq in from_indices)\n        from_indices = [subseq + [self.char_vocab.mask_index] * (max_length - len(subseq)) for subseq in from_indices]\n        to_indices = [subseq + [self.char_vocab.mask_index] * (max_length - len(subseq)) for subseq in to_indices]\n\n\n        from_vectors = np.array(from_indices, dtype=np.int64)\n        to_vectors = np.array(to_indices, dtype=np.int64)\n        assert from_vectors.shape == to_vectors.shape\n        \n        return from_vectors, to_vectors\n\n\n\n    @classmethod\n    def from_dataframe(cls, poem_df):\n        \"\"\"Instantiate the vectorizer from the dataset dataframe\n        \n        Args:\n            poem_df (pandas.DataFrame): the poem dataset\n        Returns:\n            an instance of the PoemVectorizer\n        \"\"\"\n        char_vocab = SequenceVocabulary()\n        #author_vocab = Vocabulary()\n\n        for index, row in poem_df.iterrows():\n            for char in row.poem_body:\n                char_vocab.add_token(char)\n            #author_vocab.add_token(row.author)\n\n        return cls(char_vocab)\n\n    @classmethod\n    def from_serializable(cls, contents):\n        \"\"\"Instantiate the vectorizer from saved contents\n        \n        Args:\n            contents (dict): a dict holding two vocabularies for this vectorizer\n                This dictionary is created using `vectorizer.to_serializable()`\n        Returns:\n            an instance of PoemVectorizer\n        \"\"\"\n        \n        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n        #author_vocab =  Vocabulary.from_serializable(contents['author_vocab'])\n        \n\n        return cls(char_vocab=char_vocab)\n\n    def to_serializable(self):\n        \"\"\" Returns the serializable contents \"\"\"\n        return {'char_vocab': self.char_vocab.to_serializable()}#, \n                #'author_vocab': self.author_vocab.to_serializable()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.948287Z","iopub.execute_input":"2023-08-23T06:36:13.948607Z","iopub.status.idle":"2023-08-23T06:36:13.969950Z","shell.execute_reply.started":"2023-08-23T06:36:13.948578Z","shell.execute_reply":"2023-08-23T06:36:13.968564Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Extending Pytorch's Dataset class to handle our dataset\n    In charge of the loading the dataset, and creating batches from it so our model can train on it.","metadata":{}},{"cell_type":"code","source":"class PoemDataset(Dataset):\n    def __init__(self, poem_df, vectorizer, max_seq_length=-1, subsequence_length=64, stride=1):\n        \"\"\"\n        Args:\n            poem_df (pandas.DataFrame): the dataset\n            vectorizer (PoemVectorizer): vectorizer instatiated from dataset\n            max_seq_length (int): the maximum length of the sequence of indices\n            subsequence_length (int): subdividing the sequence of indices into smaller ones of length\n                                      subsequence_length to prevent vanishing gradient\n            stride (int): the amount of overlapping in the subsequence of indices\n        \"\"\"\n        self.poem_df = poem_df \n        self._vectorizer = vectorizer\n\n        #self._max_seq_length = max(map(len, self.poem_df.poem_body)) + 2\n        self._max_seq_length = max_seq_length\n        self._subsequence_length = subsequence_length\n        self._stride = stride\n        #print(\"seq length \" , self._max_seq_length)\n        self.total_char_length = 0\n\n        self.train_df = self.poem_df[self.poem_df.split=='train']\n        self.train_size = len(self.train_df)\n\n        self.val_df = self.poem_df[self.poem_df.split=='val']\n        self.validation_size = len(self.val_df)\n\n        self.test_df = self.poem_df[self.poem_df.split=='test']\n        self.test_size = len(self.test_df)\n\n        self._lookup_dict = {'train': (self.train_df, self.train_size), \n                             'val': (self.val_df, self.validation_size), \n                             'test': (self.test_df, self.test_size)}\n\n        self.set_split('train')\n        \n    @classmethod\n    def load_dataset_and_make_vectorizer(cls, poem_csv,subsequence_length=64,stride=1):\n        \"\"\"Load dataset and make a new vectorizer from scratch\n        \n        Args:\n            poem_csv (str): location of the dataset\n        Returns:\n            an instance of PoemDataset\n        \"\"\"\n        \n        poem_df = pd.read_csv(poem_csv)\n        return cls(poem_df, PoemVectorizer.from_dataframe(poem_df),subsequence_length=subsequence_length,stride=stride)\n        \n    @classmethod\n    def load_dataset_and_load_vectorizer(cls, poem_csv, vectorizer_filepath):\n        \"\"\"Load dataset and the corresponding vectorizer. \n        Used in the case in the vectorizer has been cached for re-use\n        \n        Args:\n            poem_csv (str): location of the dataset\n            vectorizer_filepath (str): location of the saved vectorizer\n        Returns:\n            an instance of PoemDataset\n        \"\"\"\n        poem_df = pd.read_csv(poem_csv)\n        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n        return cls(poem_df, vectorizer)\n\n    @staticmethod\n    def load_vectorizer_only(vectorizer_filepath):\n        \"\"\"a static method for loading the vectorizer from file\n        \n        Args:\n            vectorizer_filepath (str): the location of the serialized vectorizer\n        Returns:\n            an instance of PoemVectorizer\n        \"\"\"\n        with open(vectorizer_filepath) as fp:\n            return PoemVectorizer.from_serializable(json.load(fp))\n\n    def save_vectorizer(self, vectorizer_filepath):\n        \"\"\"saves the vectorizer to disk using json\n        \n        Args:\n            vectorizer_filepath (str): the location to save the vectorizer\n        \"\"\"\n        with open(vectorizer_filepath, \"w\") as fp:\n            json.dump(self._vectorizer.to_serializable(), fp)\n\n    def get_vectorizer(self):\n        \"\"\" returns the vectorizer \"\"\"\n        return self._vectorizer\n\n    def set_split(self, split=\"train\"):\n        self._target_split = split\n        self._target_df, self._target_size = self._lookup_dict[split]\n\n    def __len__(self):\n        return self._target_size\n\n        \n    def __getitem__(self, index):\n        row = self.poem_df.iloc[index]\n\n        # Get the poem text\n        poem = row.poem_body\n\n        # Vectorize the poem into input and target tensors\n        from_vectors, to_vectors = self._vectorizer.vectorize(poem, self._subsequence_length, self._stride)\n\n        samples = [{'x_data': from_vector, 'y_target': to_vector} for from_vector, to_vector in zip(from_vectors, to_vectors)]\n\n        return samples\n\n    def get_num_batches(self, batch_size):\n        \"\"\"Given a batch size, return the number of batches in the dataset\n        \n        Args:\n            batch_size (int)\n        Returns:\n            number of batches in the dataset\n        \"\"\"\n        if self.total_char_length <= 0:\n            self.total_char_length = sum(map(len, self.poem_df.poem_body))\n        \n        return self.total_char_length // (batch_size*self._subsequence_length)\n    \n\ndef generate_batches(dataset, batch_size, subsequence_length, stride, shuffle=True, drop_last=True, device=\"cpu\"): \n    \"\"\"\n    A generator function which wraps the PyTorch DataLoader. It will \n    ensure each tensor is on the correct device location.\n    \"\"\"\n    dataloader = DataLoader(\n        dataset=dataset, \n        batch_size=batch_size,\n        shuffle=shuffle, \n        drop_last=drop_last\n        # Other DataLoader parameters can be added here, e.g., num_workers, pin_memory, etc.\n    )\n    \n    for data_list in dataloader:\n        for data_dict in data_list:\n            out_data_dict = {}\n            for name,tensor in data_dict.items():\n                out_data_dict[name] = tensor.to(device)\n            yield out_data_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.971870Z","iopub.execute_input":"2023-08-23T06:36:13.972904Z","iopub.status.idle":"2023-08-23T06:36:13.996741Z","shell.execute_reply.started":"2023-08-23T06:36:13.972864Z","shell.execute_reply":"2023-08-23T06:36:13.995214Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# PoetryGenerationModel","metadata":{}},{"cell_type":"code","source":"class PoetryGenerationModel(nn.Module):\n    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size, \n                 batch_first=True, padding_idx=0, dropout_p=0.5):\n        \"\"\"\n        Args:\n            char_embedding_size (int): The size of the character embeddings\n            char_vocab_size (int): The number of characters to embed\n            rnn_hidden_size (int): The size of the RNN's hidden state\n            batch_first (bool): Informs whether the input tensors will \n                have batch or the sequence on the 0th dimension\n            padding_idx (int): The index for the tensor padding; \n                see torch.nn.Embedding\n            dropout_p (float): the probability of zeroing activations using\n                the dropout method.  higher means more likely to zero.\n        \"\"\"\n        super(PoetryGenerationModel, self).__init__()\n        \n        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n                                     embedding_dim=char_embedding_size,\n                                     padding_idx=padding_idx)\n\n        self.rnn = nn.GRU(input_size=char_embedding_size, \n                          hidden_size=rnn_hidden_size,\n                          batch_first=batch_first)\n        \n        \n        self.fc = nn.Linear(in_features=rnn_hidden_size, \n                            out_features=char_vocab_size)\n        \n        self._dropout_p = dropout_p\n\n    def forward(self, x_in, apply_softmax=False):\n        \"\"\"The forward pass of the model\n        \n        Args:\n            x_in (torch.Tensor): an input data tensor. \n                x_in.shape should be (batch, input_dim)\n            apply_softmax (bool): a flag for the softmax activation\n                should be false if used with the Cross Entropy losses\n        Returns:\n            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n        \"\"\"\n        x_embedded = self.char_emb(x_in)\n\n        y_out, _ = self.rnn(x_embedded)\n\n        batch_size, seq_size, feat_size = y_out.shape\n        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n\n        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n                         \n        if apply_softmax:\n            y_out = F.softmax(y_out, dim=1)\n            \n        new_feat_size = y_out.shape[-1]\n        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n            \n        return y_out","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:13.998036Z","iopub.execute_input":"2023-08-23T06:36:13.998455Z","iopub.status.idle":"2023-08-23T06:36:14.020009Z","shell.execute_reply.started":"2023-08-23T06:36:13.998427Z","shell.execute_reply":"2023-08-23T06:36:14.018668Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class PoetryGenerator:\n    \"\"\"\n    Class to handle training and generation \n    \"\"\"\n    def __init__(self, args):\n        \"\"\"\n        Args:\n            args: all the parameters and hyperparameters of the model and training\n        \"\"\"\n        self.args = args\n        if self.args.reload_from_files:\n            # training from a checkpoint\n            self.dataset = PoemDataset.load_dataset_and_load_vectorizer(args.poem_csv,\n                                                                      args.vectorizer_file)\n        else:\n            # create dataset and vectorizer\n            self.dataset = PoemDataset.load_dataset_and_make_vectorizer(args.poem_csv, args.subsequence_length, args.stride)\n            self.dataset.save_vectorizer(args.vectorizer_file)\n\n        self.vectorizer = self.dataset.get_vectorizer()\n\n        self.model = PoetryGenerationModel(char_embedding_size=args.char_embedding_size,\n                                       char_vocab_size=len(self.vectorizer.char_vocab),\n                                       rnn_hidden_size=args.rnn_hidden_size,\n                                       padding_idx=self.vectorizer.char_vocab.mask_index)\n        print(\"Subsequence length \", self.dataset._subsequence_length)\n    \n    def sample_from_model(self, num_samples=1, sample_size=20, seed_indices=None, temperature=0.8):\n        begin_seq_index = torch.tensor([self.vectorizer.char_vocab.begin_seq_index for _ in range(num_samples)], dtype=torch.int64).unsqueeze(dim=1)\n\n        # If seed_indices are provided, replace the hidden state after the begin_seq\n        if seed_indices is not None:\n            seed_indices = seed_indices.unsqueeze(0)  # Add batch dimension      \n            h_t = seed_indices\n        else:\n            h_t = None\n\n        indices = [begin_seq_index]\n        for time_step in range(sample_size):\n            x_t = indices[time_step]\n            x_emb_t = self.model.char_emb(x_t)\n            if h_t is not None:\n                h_t = h_t.to(x_emb_t.dtype)\n            rnn_out_t, h_t = self.model.rnn(x_emb_t, h_t)\n            prediction_vector = self.model.fc(rnn_out_t.squeeze(dim=1))\n            probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n            new_indices = torch.multinomial(probability_vector, num_samples=1)\n            indices.append(new_indices)\n\n        indices = torch.cat(indices, dim=1)  # Concatenate the list of tensors along the second dimension\n        return indices\n\n    def decode_samples(self, sampled_indices):\n        \"\"\"Transform indices into the string form of a poem\n\n        Args:\n            sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n            \n        \"\"\"\n        decoded_poem = []\n        vocab = self.vectorizer.char_vocab\n\n        for sample_index in range(sampled_indices.shape[0]):\n            poem = \"\"\n            for time_step in range(sampled_indices.shape[1]):\n                sample_item = sampled_indices[sample_index, time_step].item()\n                if sample_item == vocab.begin_seq_index:\n                    continue\n                elif sample_item == vocab.end_seq_index:\n                    break\n                else:\n                    poem += vocab.lookup_index(sample_item)\n            decoded_poem.append(poem)\n        return decoded_poem\n    \n    def make_train_state(self):\n        return {'stop_early': False,\n                'early_stopping_step': 0,\n                'early_stopping_best_val': 1e8,\n                'learning_rate': self.args.learning_rate,\n                'epoch_index': 0,\n                'train_loss': [],\n                'train_acc': [],\n                'val_loss': [],\n                'val_acc': [],\n                'test_loss': -1,\n                'test_acc': -1,\n                'model_filename': self.args.model_state_file}\n\n    def update_train_state(self, train_state):\n        \"\"\"Handle the training state updates.\n        Components:\n         - Early Stopping: Prevent overfitting.\n         - Model Checkpoint: Model is saved if the model is better\n\n        :param train_state: a dictionary representing the training state values\n        :returns:\n            a new train_state\n        \"\"\"\n\n        # Save one model at least\n        if train_state['epoch_index'] == 0:\n            torch.save(self.model.state_dict(), train_state['model_filename'])\n            train_state['stop_early'] = False\n\n        # Save model if performance improved\n        elif train_state['epoch_index'] >= 1:\n            loss_tm1, loss_t = train_state['val_loss'][-2:]\n\n            # If loss worsened\n            if loss_t >= loss_tm1:\n                # Update step\n                train_state['early_stopping_step'] += 1\n            # Loss decreased\n            else:\n                # Save the best model\n                if loss_t < train_state['early_stopping_best_val']:\n                    torch.save(self.model.state_dict(), train_state['model_filename'])\n                    train_state['early_stopping_best_val'] = loss_t\n\n                # Reset early stopping step\n                train_state['early_stopping_step'] = 0\n\n            # Stop early ?\n            train_state['stop_early'] = \\\n                train_state['early_stopping_step'] >= self.args.early_stopping_criteria\n\n        return train_state\n\n    def normalize_sizes(self, y_pred, y_true):\n        \"\"\"Normalize tensor sizes\n\n        Args:\n            y_pred (torch.Tensor): the output of the model\n                If a 3-dimensional tensor, reshapes to a matrix\n            y_true (torch.Tensor): the target predictions\n                If a matrix, reshapes to be a vector\n        \"\"\"\n        if len(y_pred.size()) == 3:\n            y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n        if len(y_true.size()) == 2:\n            y_true = y_true.contiguous().view(-1)\n        return y_pred, y_true\n\n    def compute_accuracy(self, y_pred, y_true, mask_index):\n        y_pred, y_true = self.normalize_sizes(y_pred, y_true)\n\n        _, y_pred_indices = y_pred.max(dim=1)\n\n        correct_indices = torch.eq(y_pred_indices, y_true).float()\n        valid_indices = torch.ne(y_true, mask_index).float()\n\n        n_correct = (correct_indices * valid_indices).sum().item()\n        n_valid = valid_indices.sum().item()\n\n        return n_correct / n_valid * 100\n\n    def sequence_loss(self, y_pred, y_true, mask_index):\n        y_pred, y_true = self.normalize_sizes(y_pred, y_true)\n        return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n    \n    def training(self):\n        \"\"\"\n        Training loop\n        \"\"\"\n        mask_index = self.vectorizer.char_vocab.mask_index\n\n        self.model = self.model.to(self.args.device)\n\n        optimizer = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n                                               mode='min', factor=0.5,\n                                               patience=1)\n        self.train_state = self.make_train_state()\n\n        try:\n            for epoch_index in range(self.args.num_epochs):\n                self.train_state['epoch_index'] = epoch_index\n\n                # Print current epoch and early stopping criteria\n                print(f\"Epoch {epoch_index+1}/{self.args.num_epochs}\")\n                print(f\"Early Stopping Criteria: {self.args.early_stopping_criteria}\")\n\n                # Iterate over training dataset\n\n                # setup: batch generator, set loss and acc to 0, set train mode on\n                self.dataset.set_split('train')\n                batch_generator = generate_batches(self.dataset, \n                                                   batch_size=self.args.batch_size,\n                                                   subsequence_length = self.args.subsequence_length,\n                                                   stride = self.args.stride,\n                                                   device=self.args.device)\n                running_loss = 0.0\n                running_acc = 0.0\n                self.model.train()\n                #max_idx = 0\n                #for batch_index, batch_dict in enumerate(batch_generator):\n                    #print(batch_dict)\n                    #if batch_index > max_idx:\n                        #max_idx = batch_index\n\n                for batch_index, batch_dict in enumerate(batch_generator):\n                    \n                    # the training routine is these 5 steps:\n\n                    # --------------------------------------    \n                    # step 1. zero the gradients\n                    optimizer.zero_grad()\n\n                    # step 2. compute the output\n                    y_pred = self.model(x_in=batch_dict['x_data'])\n\n                    # step 3. compute the loss\n                    loss = self.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n\n                    # step 4. use loss to produce gradients\n                    loss.backward()\n\n                    # step 5. use optimizer to take gradient step\n                    optimizer.step()\n                    # -----------------------------------------\n                    # compute the  running loss and running accuracy\n                    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n                    acc_t = self.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n                    running_acc += (acc_t - running_acc) / (batch_index + 1)\n\n                    # Print current loss and accuracy\n                    print(\n                          f\"Loss: {running_loss:.4f}, Accuracy: {running_acc:.2f}%\")\n\n                self.train_state['train_loss'].append(running_loss)\n                self.train_state['train_acc'].append(running_acc)\n\n                # Iterate over val dataset\n\n                # setup: batch generator, set loss and acc to 0; set eval mode on\n                self.dataset.set_split('val')\n                batch_generator = generate_batches(self.dataset, \n                                                   batch_size=args.batch_size,\n                                                   subsequence_length = self.args.subsequence_length,\n                                                   stride = self.args.stride,\n                                                   device=args.device)\n                running_loss = 0.\n                running_acc = 0.\n                self.model.eval()\n\n                for batch_index, batch_dict in enumerate(batch_generator):\n                    # compute the output\n                    y_pred = self.model(x_in=batch_dict['x_data'])\n\n                    # step 3. compute the loss\n                    loss = self.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n\n                    # compute the  running loss and running accuracy\n                    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n                    acc_t = self.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n                    running_acc += (acc_t - running_acc) / (batch_index + 1)\n\n                self.train_state['val_loss'].append(running_loss)\n                self.train_state['val_acc'].append(running_acc)\n\n                self.train_state = self.update_train_state(train_state=self.train_state)\n\n                scheduler.step(self.train_state['val_loss'][-1])\n\n                if self.train_state['stop_early']:\n                    break\n\n                # move model to cpu for sampling\n                self.model = self.model.cpu()\n                sampled_poem = self.decode_samples(\n                    self.sample_from_model( num_samples=2))\n                # Show results\n                print(\"-\" * 15)\n                for i in range(2):\n                    print(sampled_poem[i])\n                # move model back to whichever device it should be on\n                self.model = self.model.to(self.args.device)\n\n        except KeyboardInterrupt:\n            print(\"Exiting loop\")\n\n    def evaluate_test(self):\n        # compute the loss & accuracy on the test set using the best available model\n\n        self.model.load_state_dict(torch.load(self.train_state['model_filename']))\n\n        self.model = self.model.to(self.args.device)\n\n        self.dataset.set_split('test')\n        batch_generator = generate_batches(self.dataset, \n                                           batch_size=self.args.batch_size,\n                                           subsequence_length = self.args.subsequence_length,\n                                           stride = self.args.stride,\n                                           device=self.args.device)\n        running_acc = 0.\n        self.model.eval()\n\n        for batch_index, batch_dict in enumerate(batch_generator):\n            # compute the output\n            y_pred = self.model(x_in=batch_dict['x_data'])\n\n            # compute the loss\n            loss = self.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n\n            # compute the accuracy\n            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n\n            acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n            running_acc += (acc_t - running_acc) / (batch_index + 1)\n\n        self.train_state['test_loss'] = running_loss \n        self.train_state['test_acc'] = running_acc \n        print(\"Test loss: {};\".format(train_state['test_loss']))\n        print(\"Test Accuracy: {}\".format(train_state['test_acc']))\n\n    def generate_poem(self, seed_word, num_poems=10, sample_size=50):\n        \"\"\"\n        Generate a poem starting from a seed word.\n\n        Args:\n            seed_word (str): The seed word for poem generation.\n            num_poems (int): Number of poems to generate.\n            sample_size (int): The max length of the samples.\n\n        Returns:\n            None\n        \"\"\"\n        self.model = self.model.cpu()\n\n        # Vectorize the seed word\n        if seed_word:\n            seed_indices = self.vectorize_seed_word(seed_word, num_poems)\n\n        else:\n            seed_indices = None\n\n        # Generate poem using the initial hidden state\n        sampled_poem = self.decode_samples(\n            self.sample_from_model(num_samples=num_poems, sample_size=sample_size, seed_indices=seed_indices)\n        )\n\n        # Show generated poems\n        print(\"-\" * 15)\n        for i in range(num_poems):\n            print(sampled_poem[i])\n\n    def vectorize_seed_word(self, seed_word, num_poems):\n        \"\"\"Vectorize the seed word using the vectorizer.\n\n        Args:\n            seed_word (str): The seed word.\n\n        Returns:\n            torch.Tensor: The vectorized seed word indices.\n        \"\"\"\n        seed_indices = [self.vectorizer.char_vocab.lookup_token(char) for char in seed_word]\n        # Pad the sequence to match the size of the hidden state\n        while len(seed_indices) < self.model.rnn.hidden_size:\n            seed_indices.append(self.vectorizer.char_vocab.mask_index)\n        seed_indices = torch.tensor(\n            [seed_indices for _ in range(num_poems)],\n            dtype=torch.long\n        )\n        return seed_indices\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:14.021343Z","iopub.execute_input":"2023-08-23T06:36:14.021946Z","iopub.status.idle":"2023-08-23T06:36:14.064802Z","shell.execute_reply.started":"2023-08-23T06:36:14.021913Z","shell.execute_reply":"2023-08-23T06:36:14.063058Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# General utilities","metadata":{}},{"cell_type":"code","source":"def set_seed_everywhere(seed, cuda):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed_all(seed)\n\ndef handle_dirs(dirpath):\n    if not os.path.exists(dirpath):\n        os.makedirs(dirpath)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:14.066664Z","iopub.execute_input":"2023-08-23T06:36:14.067500Z","iopub.status.idle":"2023-08-23T06:36:14.088511Z","shell.execute_reply.started":"2023-08-23T06:36:14.067466Z","shell.execute_reply":"2023-08-23T06:36:14.086695Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Settings and preparation","metadata":{}},{"cell_type":"code","source":"args = Namespace(\n    # Data and Path information\n    poem_csv=\"/kaggle/input/splitted-poems/famous_poems_split.csv\",\n    #poem_csv=\"/kaggle/input/splitted-poems/surnames_with_splits.csv\",\n    vectorizer_file=\"vectorizer.json\",\n    model_state_file=\"model.pth\",\n    save_dir=\"model_storage/ch7/model1_unconditioned_poem_generation\",\n    # Model hyper parameters\n    char_embedding_size=128,\n    rnn_hidden_size=128,\n    subsequence_length = 64,\n    stride = 1,\n    # Training hyper parameters\n    seed=1337,\n    learning_rate=0.001,\n    batch_size=1,\n    num_epochs=5,\n    early_stopping_criteria=15,\n    # Runtime options\n    catch_keyboard_interrupt=True,\n    cuda=True,\n    expand_filepaths_to_save_dir=True,\n    reload_from_files=False,\n)\n\nif args.expand_filepaths_to_save_dir:\n    args.vectorizer_file = os.path.join(args.save_dir,\n                                        args.vectorizer_file)\n\n    args.model_state_file = os.path.join(args.save_dir,\n                                         args.model_state_file)\n    \n    print(\"Expanded filepaths: \")\n    print(\"\\t{}\".format(args.vectorizer_file))\n    print(\"\\t{}\".format(args.model_state_file))\n    \n    \n# Check CUDA\nif not torch.cuda.is_available():\n    args.cuda = False\n\nargs.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n    \nprint(\"Using CUDA: {}\".format(args.cuda))\n\n# Set seed for reproducibility\nset_seed_everywhere(args.seed, args.cuda)\n\n# handle dirs\nhandle_dirs(args.save_dir)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:38:58.519722Z","iopub.execute_input":"2023-08-23T06:38:58.520125Z","iopub.status.idle":"2023-08-23T06:38:58.531878Z","shell.execute_reply.started":"2023-08-23T06:38:58.520094Z","shell.execute_reply":"2023-08-23T06:38:58.530141Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Expanded filepaths: \n\tmodel_storage/ch7/model1_unconditioned_poem_generation/vectorizer.json\n\tmodel_storage/ch7/model1_unconditioned_poem_generation/model.pth\nUsing CUDA: False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing the classes","metadata":{}},{"cell_type":"code","source":"def test_poem_dataset_class(poem_csv, vectorizer_file):\n    # Test loading dataset and vectorizer\n    dataset = PoemDataset.load_dataset_and_make_vectorizer(poem_csv)\n    assert isinstance(dataset, PoemDataset)\n\n    # Save the vectorizer\n    dataset.save_vectorizer(vectorizer_file)\n\n    # Load dataset with existing vectorizer\n    dataset = PoemDataset.load_dataset_and_load_vectorizer(poem_csv, vectorizer_file)\n    assert isinstance(dataset, PoemDataset)\n\n    # Test get_vectorizer method\n    vectorizer = dataset.get_vectorizer()\n    assert isinstance(vectorizer, PoemVectorizer)\n\n    # Test set_split and __len__ methods\n    dataset.set_split('train')\n    assert len(dataset) == dataset.train_size\n\n    dataset.set_split('val')\n    assert len(dataset) == dataset.validation_size\n\n    dataset.set_split('test')\n    assert len(dataset) == dataset.test_size\n\n    print(\"PoemDataset class tests passed.\")\n\ndef test_generate_batches(poem_csv, vectorizer_file, batch_size, device):\n    # Load dataset with existing vectorizer\n    dataset = PoemDataset.load_dataset_and_load_vectorizer(poem_csv, vectorizer_file)\n\n    # Set split and test generate_batches\n    dataset.set_split('train')\n    batches = generate_batches(dataset, batch_size, subsequence_length = 64, stride = 1, device=device)\n    max_idx = 0\n\n    for idx,batch in enumerate(batches):\n        #print(batch)\n        # Check if all tensors are on the correct device\n        for key, value in batch.items():\n            #print(len(key), value.size())\n            #print(key)\n            #print(value)\n            assert value.device.type == device\n\n    print(\"generate_batches function test passed.\")\n\n# Specify paths and parameters\npoem_csv = '/kaggle/input/splitted-poems/famous_poems_split.csv'\nvectorizer_file = 'vectorizer.json'\nbatch_size = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Run the tests\ntest_poem_dataset_class(poem_csv, vectorizer_file)\ntest_generate_batches(poem_csv, vectorizer_file, batch_size, device)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:36:14.114986Z","iopub.execute_input":"2023-08-23T06:36:14.116284Z","iopub.status.idle":"2023-08-23T06:36:16.817736Z","shell.execute_reply.started":"2023-08-23T06:36:14.116232Z","shell.execute_reply":"2023-08-23T06:36:16.815635Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"PoemDataset class tests passed.\ngenerate_batches function test passed.\n","output_type":"stream"}]},{"cell_type":"code","source":"poem_gen = PoetryGenerator(args)\npoem_gen.training()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:39:02.174621Z","iopub.execute_input":"2023-08-23T06:39:02.175087Z","iopub.status.idle":"2023-08-23T06:39:15.918078Z","shell.execute_reply.started":"2023-08-23T06:39:02.175050Z","shell.execute_reply":"2023-08-23T06:39:15.916667Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Subsequence length  64\nEpoch 1/5\nEarly Stopping Criteria: 15\nLoss: 4.3116, Accuracy: 0.00%\nLoss: 4.2625, Accuracy: 0.00%\nLoss: 4.2044, Accuracy: 2.08%\nLoss: 4.1639, Accuracy: 3.12%\nLoss: 4.1181, Accuracy: 7.19%\nLoss: 4.0814, Accuracy: 9.64%\nLoss: 4.0405, Accuracy: 12.72%\nLoss: 3.9957, Accuracy: 16.80%\nLoss: 3.9550, Accuracy: 18.58%\nLoss: 3.9123, Accuracy: 21.41%\nLoss: 3.8691, Accuracy: 23.15%\nLoss: 3.8270, Accuracy: 24.48%\nLoss: 3.7859, Accuracy: 26.08%\nLoss: 3.7419, Accuracy: 27.01%\nLoss: 3.6910, Accuracy: 28.02%\nLoss: 3.6443, Accuracy: 28.81%\nLoss: 3.6013, Accuracy: 29.32%\nLoss: 3.5568, Accuracy: 29.77%\nLoss: 3.5114, Accuracy: 30.02%\nLoss: 3.4710, Accuracy: 30.47%\nLoss: 3.4266, Accuracy: 30.65%\nLoss: 3.3853, Accuracy: 30.97%\nLoss: 3.3463, Accuracy: 31.11%\nLoss: 3.3075, Accuracy: 31.32%\nLoss: 3.2716, Accuracy: 31.50%\nLoss: 3.2402, Accuracy: 31.85%\nLoss: 3.2135, Accuracy: 31.77%\nLoss: 3.1863, Accuracy: 32.03%\nLoss: 3.1620, Accuracy: 32.22%\nLoss: 3.1388, Accuracy: 32.34%\nLoss: 3.1124, Accuracy: 32.66%\nLoss: 3.0882, Accuracy: 33.20%\nLoss: 3.0647, Accuracy: 33.52%\nLoss: 3.0415, Accuracy: 33.96%\nLoss: 3.0222, Accuracy: 34.33%\nLoss: 2.9996, Accuracy: 34.72%\nLoss: 2.9773, Accuracy: 35.14%\nLoss: 2.9577, Accuracy: 35.40%\nLoss: 2.9410, Accuracy: 35.78%\nLoss: 2.9224, Accuracy: 36.17%\nLoss: 2.9047, Accuracy: 36.47%\nLoss: 2.8888, Accuracy: 36.68%\nLoss: 2.8707, Accuracy: 37.03%\nLoss: 2.8543, Accuracy: 37.29%\nLoss: 2.8373, Accuracy: 37.57%\nLoss: 2.8213, Accuracy: 37.74%\nLoss: 2.8054, Accuracy: 38.07%\nLoss: 2.7893, Accuracy: 38.31%\nLoss: 2.7724, Accuracy: 38.52%\nLoss: 2.7572, Accuracy: 38.87%\nLoss: 2.7420, Accuracy: 39.00%\nLoss: 2.7284, Accuracy: 39.15%\nLoss: 2.7144, Accuracy: 39.42%\nLoss: 2.6992, Accuracy: 39.70%\nLoss: 2.6838, Accuracy: 40.03%\nLoss: 2.6711, Accuracy: 40.21%\nLoss: 2.6600, Accuracy: 40.38%\nLoss: 2.6493, Accuracy: 40.57%\nLoss: 2.6369, Accuracy: 40.81%\nLoss: 2.6255, Accuracy: 41.02%\nLoss: 2.6134, Accuracy: 41.21%\nLoss: 2.6020, Accuracy: 41.38%\nLoss: 2.5908, Accuracy: 41.52%\nLoss: 2.5810, Accuracy: 41.65%\nLoss: 2.5714, Accuracy: 41.71%\nLoss: 2.5587, Accuracy: 42.00%\nLoss: 2.5482, Accuracy: 42.12%\nLoss: 2.5378, Accuracy: 42.30%\nLoss: 2.5261, Accuracy: 42.55%\nLoss: 2.5151, Accuracy: 42.68%\nLoss: 2.5039, Accuracy: 42.89%\nLoss: 2.4928, Accuracy: 43.10%\nLoss: 2.4821, Accuracy: 43.21%\nLoss: 2.4727, Accuracy: 43.39%\nLoss: 2.4628, Accuracy: 43.54%\nLoss: 2.4531, Accuracy: 43.73%\nLoss: 2.4443, Accuracy: 43.81%\nLoss: 2.4362, Accuracy: 43.99%\nLoss: 2.4270, Accuracy: 44.13%\nLoss: 2.4185, Accuracy: 44.30%\nLoss: 2.4100, Accuracy: 44.46%\nLoss: 2.4018, Accuracy: 44.63%\nLoss: 2.3940, Accuracy: 44.80%\nLoss: 2.3862, Accuracy: 44.94%\nLoss: 2.3789, Accuracy: 45.04%\nLoss: 2.3722, Accuracy: 45.09%\nLoss: 2.3635, Accuracy: 45.26%\nLoss: 2.3553, Accuracy: 45.38%\nLoss: 2.3485, Accuracy: 45.51%\nLoss: 2.3417, Accuracy: 45.66%\nLoss: 2.3351, Accuracy: 45.76%\nLoss: 2.3290, Accuracy: 45.86%\nLoss: 2.3234, Accuracy: 45.93%\nLoss: 2.3177, Accuracy: 46.04%\nLoss: 2.3117, Accuracy: 46.15%\nLoss: 2.3051, Accuracy: 46.24%\nLoss: 2.2992, Accuracy: 46.36%\nLoss: 2.2923, Accuracy: 46.49%\nLoss: 2.2848, Accuracy: 46.65%\nLoss: 2.2774, Accuracy: 46.77%\nLoss: 2.2695, Accuracy: 46.98%\nLoss: 2.2622, Accuracy: 47.10%\nLoss: 2.2550, Accuracy: 47.24%\nLoss: 2.2472, Accuracy: 47.40%\nLoss: 2.2409, Accuracy: 47.50%\nLoss: 2.2354, Accuracy: 47.61%\nLoss: 2.2302, Accuracy: 47.65%\nLoss: 2.2247, Accuracy: 47.71%\nLoss: 2.2181, Accuracy: 47.86%\nLoss: 2.2118, Accuracy: 48.00%\nLoss: 2.2051, Accuracy: 48.16%\nLoss: 2.1997, Accuracy: 48.30%\nLoss: 2.1944, Accuracy: 48.34%\nLoss: 2.1884, Accuracy: 48.48%\nLoss: 2.1819, Accuracy: 48.61%\nLoss: 2.1762, Accuracy: 48.72%\nLoss: 2.1693, Accuracy: 48.86%\nLoss: 2.1640, Accuracy: 48.94%\nLoss: 2.1585, Accuracy: 49.05%\nLoss: 2.1520, Accuracy: 49.22%\nLoss: 2.1473, Accuracy: 49.25%\nLoss: 2.1422, Accuracy: 49.31%\nLoss: 2.1369, Accuracy: 49.39%\nLoss: 2.1317, Accuracy: 49.45%\nLoss: 2.1254, Accuracy: 49.59%\nLoss: 2.1197, Accuracy: 49.64%\nLoss: 2.1133, Accuracy: 49.77%\nLoss: 2.1069, Accuracy: 49.85%\nLoss: 2.1006, Accuracy: 49.95%\nLoss: 2.0948, Accuracy: 50.04%\nLoss: 2.0883, Accuracy: 50.14%\nLoss: 2.0823, Accuracy: 50.27%\nLoss: 2.0756, Accuracy: 50.38%\nLoss: 2.0702, Accuracy: 50.45%\nLoss: 2.0644, Accuracy: 50.58%\nLoss: 2.0587, Accuracy: 50.67%\nLoss: 2.0526, Accuracy: 50.78%\nLoss: 2.0459, Accuracy: 50.92%\nLoss: 2.0397, Accuracy: 51.05%\nLoss: 2.0331, Accuracy: 51.19%\nLoss: 2.0272, Accuracy: 51.33%\nLoss: 2.0204, Accuracy: 51.50%\nLoss: 2.0145, Accuracy: 51.65%\nLoss: 2.0087, Accuracy: 51.80%\nLoss: 2.0027, Accuracy: 51.93%\nLoss: 1.9974, Accuracy: 52.03%\nLoss: 1.9922, Accuracy: 52.21%\nLoss: 1.9881, Accuracy: 52.35%\nLoss: 1.9836, Accuracy: 52.47%\nLoss: 1.9802, Accuracy: 52.59%\nLoss: 1.9768, Accuracy: 52.72%\nLoss: 1.9731, Accuracy: 52.83%\nLoss: 1.9697, Accuracy: 52.96%\nLoss: 1.9656, Accuracy: 53.10%\nLoss: 1.9619, Accuracy: 53.20%\nLoss: 1.9584, Accuracy: 53.31%\nLoss: 1.9553, Accuracy: 53.40%\nLoss: 1.9511, Accuracy: 53.52%\nLoss: 1.9478, Accuracy: 53.61%\nLoss: 1.9442, Accuracy: 53.71%\nLoss: 1.9410, Accuracy: 53.80%\nLoss: 1.9378, Accuracy: 53.90%\nLoss: 1.9350, Accuracy: 54.00%\nLoss: 1.9312, Accuracy: 54.11%\nLoss: 1.9285, Accuracy: 54.19%\nLoss: 1.9246, Accuracy: 54.29%\nLoss: 1.9210, Accuracy: 54.42%\nLoss: 1.9175, Accuracy: 54.52%\nLoss: 1.9130, Accuracy: 54.63%\nLoss: 1.9082, Accuracy: 54.77%\nLoss: 1.9039, Accuracy: 54.89%\nLoss: 1.8991, Accuracy: 55.01%\nLoss: 1.8943, Accuracy: 55.12%\nLoss: 1.8902, Accuracy: 55.23%\nLoss: 1.8860, Accuracy: 55.32%\nLoss: 1.8815, Accuracy: 55.39%\nLoss: 1.8775, Accuracy: 55.46%\nLoss: 1.8730, Accuracy: 55.53%\nLoss: 1.8693, Accuracy: 55.60%\nLoss: 1.8652, Accuracy: 55.69%\nLoss: 1.8612, Accuracy: 55.79%\nLoss: 1.8576, Accuracy: 55.90%\nLoss: 1.8547, Accuracy: 55.97%\nLoss: 1.8518, Accuracy: 56.04%\nLoss: 1.8490, Accuracy: 56.12%\nLoss: 1.8454, Accuracy: 56.21%\nLoss: 1.8426, Accuracy: 56.29%\nLoss: 1.8406, Accuracy: 56.36%\nLoss: 1.8382, Accuracy: 56.46%\nLoss: 1.8358, Accuracy: 56.54%\nLoss: 1.8333, Accuracy: 56.61%\nLoss: 1.8303, Accuracy: 56.68%\nLoss: 1.8276, Accuracy: 56.75%\nLoss: 1.8242, Accuracy: 56.81%\nLoss: 1.8214, Accuracy: 56.87%\nLoss: 1.8182, Accuracy: 56.93%\nLoss: 1.8152, Accuracy: 56.99%\nLoss: 1.8124, Accuracy: 57.05%\nLoss: 1.8089, Accuracy: 57.12%\nLoss: 1.8059, Accuracy: 57.18%\nLoss: 1.8031, Accuracy: 57.25%\nLoss: 1.8011, Accuracy: 57.29%\nLoss: 1.7982, Accuracy: 57.36%\nLoss: 1.7957, Accuracy: 57.44%\nLoss: 1.7929, Accuracy: 57.52%\nLoss: 1.7898, Accuracy: 57.56%\nLoss: 1.7868, Accuracy: 57.60%\nLoss: 1.7834, Accuracy: 57.68%\nLoss: 1.7800, Accuracy: 57.73%\nLoss: 1.7765, Accuracy: 57.80%\nLoss: 1.7730, Accuracy: 57.86%\nLoss: 1.7692, Accuracy: 57.93%\nLoss: 1.7660, Accuracy: 58.01%\nLoss: 1.7628, Accuracy: 58.09%\nLoss: 1.7604, Accuracy: 58.14%\nLoss: 1.7580, Accuracy: 58.20%\nLoss: 1.7561, Accuracy: 58.24%\nLoss: 1.7538, Accuracy: 58.29%\nLoss: 1.7512, Accuracy: 58.36%\nLoss: 1.7492, Accuracy: 58.42%\nLoss: 1.7470, Accuracy: 58.45%\nLoss: 1.7449, Accuracy: 58.54%\nLoss: 1.7423, Accuracy: 58.62%\nLoss: 1.7405, Accuracy: 58.68%\nLoss: 1.7386, Accuracy: 58.71%\nLoss: 1.7370, Accuracy: 58.77%\nLoss: 1.7354, Accuracy: 58.82%\nLoss: 1.7337, Accuracy: 58.85%\nLoss: 1.7324, Accuracy: 58.92%\nLoss: 1.7312, Accuracy: 58.95%\nLoss: 1.7303, Accuracy: 58.98%\nLoss: 1.7291, Accuracy: 59.02%\nLoss: 1.7274, Accuracy: 59.07%\nLoss: 1.7263, Accuracy: 59.11%\nLoss: 1.7256, Accuracy: 59.14%\nLoss: 1.7238, Accuracy: 59.17%\nLoss: 1.7222, Accuracy: 59.20%\nLoss: 1.7203, Accuracy: 59.24%\nLoss: 1.7187, Accuracy: 59.25%\nLoss: 1.7159, Accuracy: 59.30%\nLoss: 1.7141, Accuracy: 59.34%\nLoss: 1.7123, Accuracy: 59.37%\nLoss: 1.7098, Accuracy: 59.41%\nLoss: 1.7079, Accuracy: 59.45%\nLoss: 1.7061, Accuracy: 59.50%\nLoss: 1.7038, Accuracy: 59.55%\nLoss: 1.7018, Accuracy: 59.58%\nLoss: 1.6996, Accuracy: 59.65%\nLoss: 1.6974, Accuracy: 59.70%\nLoss: 1.6945, Accuracy: 59.76%\nLoss: 1.6919, Accuracy: 59.80%\nLoss: 1.6892, Accuracy: 59.85%\nLoss: 1.6865, Accuracy: 59.92%\nLoss: 1.6834, Accuracy: 59.98%\nLoss: 1.6805, Accuracy: 60.05%\nLoss: 1.6774, Accuracy: 60.12%\nLoss: 1.6746, Accuracy: 60.16%\nLoss: 1.6720, Accuracy: 60.20%\nLoss: 1.6694, Accuracy: 60.26%\nLoss: 1.6666, Accuracy: 60.33%\nLoss: 1.6640, Accuracy: 60.40%\nLoss: 1.6620, Accuracy: 60.46%\nLoss: 1.6600, Accuracy: 60.51%\nLoss: 1.6579, Accuracy: 60.56%\nLoss: 1.6560, Accuracy: 60.61%\nLoss: 1.6536, Accuracy: 60.68%\nLoss: 1.6513, Accuracy: 60.73%\nLoss: 1.6491, Accuracy: 60.80%\nLoss: 1.6469, Accuracy: 60.85%\nLoss: 1.6446, Accuracy: 60.91%\nLoss: 1.6423, Accuracy: 60.98%\nLoss: 1.6403, Accuracy: 61.02%\nLoss: 1.6381, Accuracy: 61.07%\nLoss: 1.6356, Accuracy: 61.15%\nLoss: 1.6337, Accuracy: 61.20%\nLoss: 1.6318, Accuracy: 61.26%\nLoss: 1.6299, Accuracy: 61.33%\nLoss: 1.6277, Accuracy: 61.40%\nLoss: 1.6259, Accuracy: 61.44%\nLoss: 1.6238, Accuracy: 61.50%\nLoss: 1.6220, Accuracy: 61.55%\nLoss: 1.6194, Accuracy: 61.60%\nLoss: 1.6175, Accuracy: 61.65%\nLoss: 1.6157, Accuracy: 61.69%\nLoss: 1.6135, Accuracy: 61.74%\nLoss: 1.6119, Accuracy: 61.77%\nLoss: 1.6101, Accuracy: 61.81%\nLoss: 1.6084, Accuracy: 61.87%\nLoss: 1.6066, Accuracy: 61.92%\nLoss: 1.6051, Accuracy: 61.95%\nLoss: 1.6031, Accuracy: 61.98%\nLoss: 1.6012, Accuracy: 62.03%\nLoss: 1.5995, Accuracy: 62.06%\nLoss: 1.5974, Accuracy: 62.11%\nLoss: 1.5953, Accuracy: 62.17%\nLoss: 1.5929, Accuracy: 62.23%\nLoss: 1.5904, Accuracy: 62.30%\nLoss: 1.5880, Accuracy: 62.36%\nLoss: 1.5857, Accuracy: 62.43%\nLoss: 1.5831, Accuracy: 62.48%\nLoss: 1.5809, Accuracy: 62.54%\nLoss: 1.5786, Accuracy: 62.60%\nLoss: 1.5762, Accuracy: 62.67%\nLoss: 1.5736, Accuracy: 62.73%\nLoss: 1.5714, Accuracy: 62.79%\nLoss: 1.5690, Accuracy: 62.85%\nLoss: 1.5670, Accuracy: 62.89%\nLoss: 1.5650, Accuracy: 62.94%\nLoss: 1.5629, Accuracy: 63.01%\nLoss: 1.5611, Accuracy: 63.05%\nLoss: 1.5597, Accuracy: 63.11%\nLoss: 1.5579, Accuracy: 63.19%\nLoss: 1.5563, Accuracy: 63.24%\nLoss: 1.5547, Accuracy: 63.30%\nLoss: 1.5532, Accuracy: 63.34%\nLoss: 1.5515, Accuracy: 63.39%\nLoss: 1.5500, Accuracy: 63.42%\nLoss: 1.5481, Accuracy: 63.46%\nLoss: 1.5460, Accuracy: 63.53%\nLoss: 1.5438, Accuracy: 63.58%\nLoss: 1.5417, Accuracy: 63.64%\nLoss: 1.5397, Accuracy: 63.70%\nLoss: 1.5375, Accuracy: 63.76%\nLoss: 1.5353, Accuracy: 63.82%\nLoss: 1.5332, Accuracy: 63.89%\nLoss: 1.5311, Accuracy: 63.94%\nLoss: 1.5290, Accuracy: 63.99%\nLoss: 1.5276, Accuracy: 64.04%\nLoss: 1.5265, Accuracy: 64.09%\nLoss: 1.5251, Accuracy: 64.13%\nLoss: 1.5239, Accuracy: 64.18%\nLoss: 1.5228, Accuracy: 64.21%\nLoss: 1.5215, Accuracy: 64.25%\nLoss: 1.5206, Accuracy: 64.28%\nLoss: 1.5196, Accuracy: 64.34%\nLoss: 1.5182, Accuracy: 64.37%\nLoss: 1.5168, Accuracy: 64.40%\nLoss: 1.5154, Accuracy: 64.42%\nLoss: 1.5140, Accuracy: 64.44%\nLoss: 1.5127, Accuracy: 64.45%\nLoss: 1.5111, Accuracy: 64.48%\nLoss: 1.5098, Accuracy: 64.51%\nLoss: 1.5086, Accuracy: 64.54%\nLoss: 1.5073, Accuracy: 64.58%\nLoss: 1.5054, Accuracy: 64.62%\nLoss: 1.5044, Accuracy: 64.65%\nLoss: 1.5030, Accuracy: 64.67%\nLoss: 1.5017, Accuracy: 64.70%\nLoss: 1.5003, Accuracy: 64.73%\nLoss: 1.4989, Accuracy: 64.77%\nLoss: 1.4977, Accuracy: 64.80%\nLoss: 1.4959, Accuracy: 64.85%\nLoss: 1.4945, Accuracy: 64.89%\nLoss: 1.4930, Accuracy: 64.94%\nLoss: 1.4917, Accuracy: 64.97%\nLoss: 1.4905, Accuracy: 64.99%\nLoss: 1.4896, Accuracy: 65.01%\nLoss: 1.4887, Accuracy: 65.04%\nLoss: 1.4879, Accuracy: 65.06%\nLoss: 1.4876, Accuracy: 65.08%\nLoss: 1.4865, Accuracy: 65.12%\nLoss: 1.4857, Accuracy: 65.14%\nLoss: 1.4853, Accuracy: 65.16%\nLoss: 1.4850, Accuracy: 65.18%\nLoss: 1.4846, Accuracy: 65.20%\nLoss: 1.4837, Accuracy: 65.22%\nLoss: 1.4825, Accuracy: 65.25%\nLoss: 1.4817, Accuracy: 65.27%\nLoss: 1.4805, Accuracy: 65.30%\nLoss: 1.4793, Accuracy: 65.33%\nLoss: 1.4779, Accuracy: 65.36%\nLoss: 1.4761, Accuracy: 65.41%\nLoss: 1.4744, Accuracy: 65.45%\nLoss: 1.4728, Accuracy: 65.49%\nLoss: 1.4710, Accuracy: 65.53%\nLoss: 1.4691, Accuracy: 65.58%\nLoss: 1.4678, Accuracy: 65.62%\nLoss: 1.4660, Accuracy: 65.65%\nLoss: 1.4643, Accuracy: 65.67%\nLoss: 1.4626, Accuracy: 65.72%\nLoss: 1.4609, Accuracy: 65.76%\nLoss: 1.4595, Accuracy: 65.79%\nLoss: 1.4582, Accuracy: 65.82%\nLoss: 1.4569, Accuracy: 65.85%\nLoss: 1.4556, Accuracy: 65.88%\nLoss: 1.4547, Accuracy: 65.89%\nLoss: 1.4534, Accuracy: 65.94%\nLoss: 1.4523, Accuracy: 65.96%\nLoss: 1.4510, Accuracy: 65.99%\nLoss: 1.4496, Accuracy: 66.02%\nLoss: 1.4486, Accuracy: 66.04%\nLoss: 1.4476, Accuracy: 66.08%\nLoss: 1.4468, Accuracy: 66.10%\nLoss: 1.4459, Accuracy: 66.12%\nLoss: 1.4454, Accuracy: 66.14%\nLoss: 1.4447, Accuracy: 66.15%\nLoss: 1.4440, Accuracy: 66.16%\nLoss: 1.4434, Accuracy: 66.17%\nLoss: 1.4426, Accuracy: 66.20%\nLoss: 1.4417, Accuracy: 66.21%\nLoss: 1.4412, Accuracy: 66.22%\nLoss: 1.4403, Accuracy: 66.23%\nLoss: 1.4394, Accuracy: 66.24%\nLoss: 1.4387, Accuracy: 66.26%\nLoss: 1.4376, Accuracy: 66.26%\nLoss: 1.4362, Accuracy: 66.28%\nLoss: 1.4358, Accuracy: 66.28%\nLoss: 1.4348, Accuracy: 66.29%\nLoss: 1.4339, Accuracy: 66.30%\nLoss: 1.4332, Accuracy: 66.31%\nLoss: 1.4329, Accuracy: 66.30%\nLoss: 1.4322, Accuracy: 66.32%\nLoss: 1.4313, Accuracy: 66.32%\nLoss: 1.4300, Accuracy: 66.35%\nLoss: 1.4291, Accuracy: 66.36%\nLoss: 1.4281, Accuracy: 66.38%\nLoss: 1.4270, Accuracy: 66.40%\nLoss: 1.4256, Accuracy: 66.44%\nLoss: 1.4244, Accuracy: 66.47%\nLoss: 1.4234, Accuracy: 66.51%\nLoss: 1.4224, Accuracy: 66.53%\nLoss: 1.4212, Accuracy: 66.55%\nLoss: 1.4201, Accuracy: 66.58%\nLoss: 1.4191, Accuracy: 66.60%\nLoss: 1.4179, Accuracy: 66.64%\nLoss: 1.4169, Accuracy: 66.67%\nLoss: 1.4158, Accuracy: 66.69%\nLoss: 1.4149, Accuracy: 66.71%\nLoss: 1.4143, Accuracy: 66.72%\nLoss: 1.4134, Accuracy: 66.75%\nLoss: 1.4123, Accuracy: 66.77%\nLoss: 1.4113, Accuracy: 66.81%\nLoss: 1.4106, Accuracy: 66.82%\nLoss: 1.4099, Accuracy: 66.85%\nLoss: 1.4093, Accuracy: 66.86%\nLoss: 1.4087, Accuracy: 66.88%\nLoss: 1.4078, Accuracy: 66.90%\nLoss: 1.4071, Accuracy: 66.92%\nLoss: 1.4067, Accuracy: 66.94%\nLoss: 1.4060, Accuracy: 66.94%\nLoss: 1.4057, Accuracy: 66.95%\nLoss: 1.4054, Accuracy: 66.95%\nLoss: 1.4048, Accuracy: 66.97%\nLoss: 1.4042, Accuracy: 66.98%\nLoss: 1.4040, Accuracy: 66.98%\nLoss: 1.4034, Accuracy: 66.99%\nLoss: 1.4031, Accuracy: 67.00%\nLoss: 1.4026, Accuracy: 66.99%\nLoss: 1.4020, Accuracy: 67.00%\nLoss: 1.4017, Accuracy: 67.00%\nLoss: 1.4013, Accuracy: 67.01%\nLoss: 1.4008, Accuracy: 67.01%\nLoss: 1.4005, Accuracy: 67.02%\nLoss: 1.3998, Accuracy: 67.02%\nLoss: 1.3994, Accuracy: 67.03%\nLoss: 1.3987, Accuracy: 67.05%\nLoss: 1.3981, Accuracy: 67.06%\nLoss: 1.3973, Accuracy: 67.07%\nLoss: 1.3967, Accuracy: 67.08%\nLoss: 1.3961, Accuracy: 67.11%\nLoss: 1.3954, Accuracy: 67.14%\nLoss: 1.3947, Accuracy: 67.16%\nLoss: 1.3939, Accuracy: 67.18%\nLoss: 1.3932, Accuracy: 67.19%\nLoss: 1.3928, Accuracy: 67.20%\nLoss: 1.3922, Accuracy: 67.22%\nLoss: 1.3912, Accuracy: 67.24%\nLoss: 1.3903, Accuracy: 67.26%\nLoss: 1.3890, Accuracy: 67.30%\nLoss: 1.3878, Accuracy: 67.34%\nLoss: 1.3864, Accuracy: 67.38%\nLoss: 1.3855, Accuracy: 67.39%\nLoss: 1.3843, Accuracy: 67.42%\nLoss: 1.3831, Accuracy: 67.45%\nLoss: 1.3821, Accuracy: 67.49%\nLoss: 1.3814, Accuracy: 67.52%\nLoss: 1.3805, Accuracy: 67.55%\nLoss: 1.3796, Accuracy: 67.58%\nLoss: 1.3787, Accuracy: 67.61%\nLoss: 1.3778, Accuracy: 67.64%\nLoss: 1.3768, Accuracy: 67.67%\nLoss: 1.3758, Accuracy: 67.69%\nLoss: 1.3747, Accuracy: 67.71%\nLoss: 1.3736, Accuracy: 67.75%\nLoss: 1.3724, Accuracy: 67.76%\nLoss: 1.3712, Accuracy: 67.78%\nLoss: 1.3703, Accuracy: 67.80%\nLoss: 1.3693, Accuracy: 67.81%\nLoss: 1.3681, Accuracy: 67.84%\nLoss: 1.3669, Accuracy: 67.85%\nLoss: 1.3662, Accuracy: 67.86%\nLoss: 1.3650, Accuracy: 67.88%\nLoss: 1.3641, Accuracy: 67.90%\nLoss: 1.3628, Accuracy: 67.94%\nLoss: 1.3619, Accuracy: 67.96%\nLoss: 1.3608, Accuracy: 67.99%\nLoss: 1.3596, Accuracy: 68.01%\nLoss: 1.3584, Accuracy: 68.04%\nLoss: 1.3573, Accuracy: 68.07%\nLoss: 1.3561, Accuracy: 68.10%\nLoss: 1.3549, Accuracy: 68.13%\nLoss: 1.3534, Accuracy: 68.16%\nLoss: 1.3522, Accuracy: 68.19%\nLoss: 1.3509, Accuracy: 68.23%\nLoss: 1.3495, Accuracy: 68.26%\nLoss: 1.3483, Accuracy: 68.29%\nLoss: 1.3468, Accuracy: 68.33%\nLoss: 1.3455, Accuracy: 68.37%\nLoss: 1.3440, Accuracy: 68.41%\nLoss: 1.3425, Accuracy: 68.44%\nLoss: 1.3412, Accuracy: 68.47%\nLoss: 1.3399, Accuracy: 68.50%\nLoss: 1.3385, Accuracy: 68.53%\nLoss: 1.3372, Accuracy: 68.55%\nLoss: 1.3359, Accuracy: 68.57%\nLoss: 1.3346, Accuracy: 68.61%\nLoss: 1.3332, Accuracy: 68.63%\nLoss: 1.3319, Accuracy: 68.66%\nLoss: 1.3305, Accuracy: 68.69%\nLoss: 1.3289, Accuracy: 68.73%\nLoss: 1.3272, Accuracy: 68.77%\nLoss: 1.3257, Accuracy: 68.80%\nLoss: 1.3241, Accuracy: 68.84%\nLoss: 1.3224, Accuracy: 68.88%\nLoss: 1.3207, Accuracy: 68.93%\nLoss: 1.3191, Accuracy: 68.96%\nLoss: 1.3175, Accuracy: 69.00%\nLoss: 1.3162, Accuracy: 69.03%\nLoss: 1.3145, Accuracy: 69.07%\nLoss: 1.3129, Accuracy: 69.11%\nLoss: 1.3116, Accuracy: 69.13%\nLoss: 1.3105, Accuracy: 69.16%\nLoss: 1.3093, Accuracy: 69.19%\nLoss: 1.3081, Accuracy: 69.21%\nLoss: 1.3068, Accuracy: 69.23%\nLoss: 1.3056, Accuracy: 69.26%\nLoss: 1.3042, Accuracy: 69.29%\nLoss: 1.3032, Accuracy: 69.32%\nLoss: 1.3021, Accuracy: 69.35%\nLoss: 1.3011, Accuracy: 69.38%\nLoss: 1.3003, Accuracy: 69.40%\nLoss: 1.2997, Accuracy: 69.44%\nLoss: 1.2987, Accuracy: 69.46%\nLoss: 1.2981, Accuracy: 69.49%\nLoss: 1.2971, Accuracy: 69.51%\nLoss: 1.2963, Accuracy: 69.53%\nLoss: 1.2955, Accuracy: 69.55%\nLoss: 1.2947, Accuracy: 69.57%\nLoss: 1.2940, Accuracy: 69.60%\nLoss: 1.2933, Accuracy: 69.61%\nLoss: 1.2929, Accuracy: 69.63%\nLoss: 1.2922, Accuracy: 69.64%\nLoss: 1.2917, Accuracy: 69.66%\nLoss: 1.2914, Accuracy: 69.68%\nLoss: 1.2911, Accuracy: 69.68%\nLoss: 1.2909, Accuracy: 69.69%\nLoss: 1.2903, Accuracy: 69.71%\nLoss: 1.2902, Accuracy: 69.71%\nLoss: 1.2897, Accuracy: 69.72%\nLoss: 1.2893, Accuracy: 69.73%\nLoss: 1.2885, Accuracy: 69.74%\nLoss: 1.2877, Accuracy: 69.76%\nLoss: 1.2870, Accuracy: 69.76%\nLoss: 1.2864, Accuracy: 69.77%\nLoss: 1.2855, Accuracy: 69.78%\nLoss: 1.2847, Accuracy: 69.80%\nLoss: 1.2841, Accuracy: 69.81%\nLoss: 1.2836, Accuracy: 69.82%\nLoss: 1.2828, Accuracy: 69.84%\nLoss: 1.2821, Accuracy: 69.85%\nLoss: 1.2814, Accuracy: 69.86%\nLoss: 1.2809, Accuracy: 69.88%\nLoss: 1.2803, Accuracy: 69.89%\nLoss: 1.2799, Accuracy: 69.91%\nLoss: 1.2795, Accuracy: 69.92%\nLoss: 1.2790, Accuracy: 69.94%\nLoss: 1.2787, Accuracy: 69.95%\nLoss: 1.2781, Accuracy: 69.97%\nLoss: 1.2779, Accuracy: 69.97%\nLoss: 1.2777, Accuracy: 69.99%\nLoss: 1.2774, Accuracy: 70.00%\nLoss: 1.2772, Accuracy: 70.01%\nLoss: 1.2770, Accuracy: 70.03%\nLoss: 1.2769, Accuracy: 70.03%\nLoss: 1.2769, Accuracy: 70.05%\nLoss: 1.2766, Accuracy: 70.07%\nLoss: 1.2767, Accuracy: 70.08%\nLoss: 1.2765, Accuracy: 70.09%\nLoss: 1.2763, Accuracy: 70.10%\nLoss: 1.2760, Accuracy: 70.11%\nLoss: 1.2757, Accuracy: 70.12%\nLoss: 1.2752, Accuracy: 70.13%\nLoss: 1.2749, Accuracy: 70.14%\nLoss: 1.2747, Accuracy: 70.15%\nLoss: 1.2743, Accuracy: 70.17%\nLoss: 1.2741, Accuracy: 70.17%\nLoss: 1.2737, Accuracy: 70.18%\nLoss: 1.2733, Accuracy: 70.20%\nLoss: 1.2731, Accuracy: 70.21%\nLoss: 1.2727, Accuracy: 70.22%\nLoss: 1.2722, Accuracy: 70.22%\nLoss: 1.2719, Accuracy: 70.22%\nLoss: 1.2717, Accuracy: 70.23%\nLoss: 1.2714, Accuracy: 70.24%\nLoss: 1.2711, Accuracy: 70.24%\nLoss: 1.2708, Accuracy: 70.24%\nLoss: 1.2705, Accuracy: 70.24%\nLoss: 1.2700, Accuracy: 70.25%\nLoss: 1.2698, Accuracy: 70.24%\nLoss: 1.2694, Accuracy: 70.25%\nLoss: 1.2692, Accuracy: 70.26%\nLoss: 1.2690, Accuracy: 70.26%\nLoss: 1.2688, Accuracy: 70.27%\nLoss: 1.2685, Accuracy: 70.28%\nLoss: 1.2683, Accuracy: 70.28%\nLoss: 1.2679, Accuracy: 70.29%\nLoss: 1.2676, Accuracy: 70.30%\nLoss: 1.2675, Accuracy: 70.30%\nLoss: 1.2672, Accuracy: 70.31%\nLoss: 1.2664, Accuracy: 70.33%\nLoss: 1.2661, Accuracy: 70.33%\nLoss: 1.2657, Accuracy: 70.34%\nLoss: 1.2653, Accuracy: 70.35%\nLoss: 1.2648, Accuracy: 70.35%\nLoss: 1.2643, Accuracy: 70.35%\nLoss: 1.2640, Accuracy: 70.36%\nLoss: 1.2634, Accuracy: 70.37%\nLoss: 1.2630, Accuracy: 70.37%\nLoss: 1.2625, Accuracy: 70.38%\nLoss: 1.2616, Accuracy: 70.40%\nLoss: 1.2608, Accuracy: 70.41%\nLoss: 1.2599, Accuracy: 70.42%\nLoss: 1.2592, Accuracy: 70.43%\nLoss: 1.2584, Accuracy: 70.45%\nLoss: 1.2574, Accuracy: 70.47%\nLoss: 1.2565, Accuracy: 70.49%\nLoss: 1.2558, Accuracy: 70.51%\nLoss: 1.2552, Accuracy: 70.52%\nLoss: 1.2545, Accuracy: 70.54%\nLoss: 1.2540, Accuracy: 70.56%\nLoss: 1.2534, Accuracy: 70.57%\nLoss: 1.2526, Accuracy: 70.58%\nLoss: 1.2519, Accuracy: 70.60%\nLoss: 1.2512, Accuracy: 70.61%\nLoss: 1.2505, Accuracy: 70.63%\nLoss: 1.2497, Accuracy: 70.65%\nLoss: 1.2488, Accuracy: 70.66%\nLoss: 1.2480, Accuracy: 70.67%\nLoss: 1.2471, Accuracy: 70.70%\nLoss: 1.2463, Accuracy: 70.72%\nLoss: 1.2455, Accuracy: 70.73%\nLoss: 1.2449, Accuracy: 70.74%\nLoss: 1.2443, Accuracy: 70.76%\nLoss: 1.2434, Accuracy: 70.77%\nLoss: 1.2426, Accuracy: 70.80%\nLoss: 1.2419, Accuracy: 70.82%\nLoss: 1.2413, Accuracy: 70.82%\nLoss: 1.2408, Accuracy: 70.83%\nLoss: 1.2400, Accuracy: 70.86%\nLoss: 1.2391, Accuracy: 70.87%\nLoss: 1.2383, Accuracy: 70.88%\nLoss: 1.2376, Accuracy: 70.89%\nLoss: 1.2368, Accuracy: 70.91%\nLoss: 1.2361, Accuracy: 70.92%\nLoss: 1.2353, Accuracy: 70.93%\nLoss: 1.2347, Accuracy: 70.95%\nLoss: 1.2342, Accuracy: 70.96%\nLoss: 1.2336, Accuracy: 70.97%\nLoss: 1.2331, Accuracy: 70.99%\nLoss: 1.2325, Accuracy: 71.01%\nLoss: 1.2323, Accuracy: 71.02%\nLoss: 1.2319, Accuracy: 71.03%\nLoss: 1.2315, Accuracy: 71.04%\nLoss: 1.2313, Accuracy: 71.05%\nLoss: 1.2309, Accuracy: 71.07%\nLoss: 1.2306, Accuracy: 71.08%\nLoss: 1.2302, Accuracy: 71.08%\nLoss: 1.2301, Accuracy: 71.09%\nLoss: 1.2298, Accuracy: 71.10%\nLoss: 1.2295, Accuracy: 71.10%\nLoss: 1.2292, Accuracy: 71.11%\nLoss: 1.2288, Accuracy: 71.12%\nLoss: 1.2283, Accuracy: 71.13%\nLoss: 1.2279, Accuracy: 71.13%\nLoss: 1.2274, Accuracy: 71.14%\nLoss: 1.2267, Accuracy: 71.16%\nLoss: 1.2260, Accuracy: 71.18%\nLoss: 1.2253, Accuracy: 71.19%\nLoss: 1.2246, Accuracy: 71.20%\nLoss: 1.2239, Accuracy: 71.23%\nLoss: 1.2232, Accuracy: 71.24%\nLoss: 1.2225, Accuracy: 71.25%\nLoss: 1.2221, Accuracy: 71.25%\nLoss: 1.2216, Accuracy: 71.26%\nLoss: 1.2212, Accuracy: 71.27%\nLoss: 1.2208, Accuracy: 71.28%\nLoss: 1.2205, Accuracy: 71.29%\nLoss: 1.2201, Accuracy: 71.30%\nLoss: 1.2198, Accuracy: 71.31%\nLoss: 1.2194, Accuracy: 71.33%\nLoss: 1.2190, Accuracy: 71.33%\nLoss: 1.2189, Accuracy: 71.33%\nLoss: 1.2183, Accuracy: 71.35%\nLoss: 1.2176, Accuracy: 71.37%\nLoss: 1.2169, Accuracy: 71.39%\nLoss: 1.2165, Accuracy: 71.40%\nLoss: 1.2162, Accuracy: 71.42%\nLoss: 1.2156, Accuracy: 71.44%\nLoss: 1.2154, Accuracy: 71.45%\nLoss: 1.2150, Accuracy: 71.46%\nLoss: 1.2146, Accuracy: 71.48%\nLoss: 1.2140, Accuracy: 71.49%\nLoss: 1.2139, Accuracy: 71.51%\nLoss: 1.2137, Accuracy: 71.51%\nLoss: 1.2135, Accuracy: 71.52%\nLoss: 1.2133, Accuracy: 71.53%\nLoss: 1.2131, Accuracy: 71.53%\nLoss: 1.2130, Accuracy: 71.53%\nLoss: 1.2129, Accuracy: 71.53%\nLoss: 1.2127, Accuracy: 71.54%\nLoss: 1.2126, Accuracy: 71.54%\nLoss: 1.2125, Accuracy: 71.55%\nLoss: 1.2124, Accuracy: 71.55%\nLoss: 1.2125, Accuracy: 71.56%\nLoss: 1.2125, Accuracy: 71.56%\nLoss: 1.2126, Accuracy: 71.55%\nLoss: 1.2128, Accuracy: 71.54%\nLoss: 1.2127, Accuracy: 71.54%\nLoss: 1.2127, Accuracy: 71.53%\nLoss: 1.2129, Accuracy: 71.53%\nLoss: 1.2131, Accuracy: 71.52%\nLoss: 1.2133, Accuracy: 71.52%\nLoss: 1.2132, Accuracy: 71.52%\nLoss: 1.2132, Accuracy: 71.52%\nLoss: 1.2133, Accuracy: 71.52%\nLoss: 1.2133, Accuracy: 71.52%\nLoss: 1.2135, Accuracy: 71.51%\nLoss: 1.2138, Accuracy: 71.51%\nLoss: 1.2138, Accuracy: 71.51%\nLoss: 1.2139, Accuracy: 71.51%\nLoss: 1.2143, Accuracy: 71.50%\nLoss: 1.2142, Accuracy: 71.50%\nLoss: 1.2142, Accuracy: 71.51%\nLoss: 1.2141, Accuracy: 71.51%\nLoss: 1.2142, Accuracy: 71.50%\nLoss: 1.2143, Accuracy: 71.50%\nLoss: 1.2143, Accuracy: 71.49%\nLoss: 1.2141, Accuracy: 71.50%\nLoss: 1.2139, Accuracy: 71.50%\nLoss: 1.2135, Accuracy: 71.51%\nLoss: 1.2130, Accuracy: 71.52%\nLoss: 1.2126, Accuracy: 71.54%\nLoss: 1.2122, Accuracy: 71.54%\nLoss: 1.2117, Accuracy: 71.55%\nLoss: 1.2113, Accuracy: 71.56%\nLoss: 1.2107, Accuracy: 71.57%\nLoss: 1.2100, Accuracy: 71.58%\nLoss: 1.2096, Accuracy: 71.59%\nLoss: 1.2091, Accuracy: 71.61%\nLoss: 1.2085, Accuracy: 71.63%\nLoss: 1.2081, Accuracy: 71.64%\nLoss: 1.2075, Accuracy: 71.66%\nLoss: 1.2069, Accuracy: 71.67%\nLoss: 1.2062, Accuracy: 71.69%\nLoss: 1.2056, Accuracy: 71.70%\nLoss: 1.2049, Accuracy: 71.71%\nLoss: 1.2041, Accuracy: 71.73%\nLoss: 1.2033, Accuracy: 71.74%\nLoss: 1.2025, Accuracy: 71.76%\nLoss: 1.2017, Accuracy: 71.78%\nLoss: 1.2010, Accuracy: 71.80%\nLoss: 1.2000, Accuracy: 71.83%\nLoss: 1.1992, Accuracy: 71.84%\nLoss: 1.1983, Accuracy: 71.86%\nLoss: 1.1975, Accuracy: 71.88%\nLoss: 1.1967, Accuracy: 71.89%\nLoss: 1.1959, Accuracy: 71.91%\nLoss: 1.1951, Accuracy: 71.93%\nLoss: 1.1945, Accuracy: 71.95%\nLoss: 1.1937, Accuracy: 71.96%\nLoss: 1.1929, Accuracy: 71.98%\nLoss: 1.1920, Accuracy: 72.00%\nLoss: 1.1911, Accuracy: 72.02%\nLoss: 1.1901, Accuracy: 72.04%\nLoss: 1.1890, Accuracy: 72.06%\nLoss: 1.1880, Accuracy: 72.08%\nLoss: 1.1869, Accuracy: 72.11%\nLoss: 1.1860, Accuracy: 72.13%\nLoss: 1.1851, Accuracy: 72.15%\nLoss: 1.1842, Accuracy: 72.17%\nLoss: 1.1833, Accuracy: 72.19%\nLoss: 1.1824, Accuracy: 72.21%\nLoss: 1.1815, Accuracy: 72.24%\nLoss: 1.1805, Accuracy: 72.26%\nLoss: 1.1797, Accuracy: 72.28%\nLoss: 1.1788, Accuracy: 72.30%\nLoss: 1.1781, Accuracy: 72.32%\nLoss: 1.1773, Accuracy: 72.33%\nLoss: 1.1766, Accuracy: 72.34%\nLoss: 1.1758, Accuracy: 72.36%\nLoss: 1.1750, Accuracy: 72.37%\nLoss: 1.1742, Accuracy: 72.39%\nLoss: 1.1733, Accuracy: 72.40%\nLoss: 1.1724, Accuracy: 72.42%\nLoss: 1.1715, Accuracy: 72.45%\nLoss: 1.1706, Accuracy: 72.46%\nLoss: 1.1697, Accuracy: 72.48%\nLoss: 1.1688, Accuracy: 72.50%\nLoss: 1.1678, Accuracy: 72.53%\nLoss: 1.1671, Accuracy: 72.55%\nLoss: 1.1664, Accuracy: 72.56%\nLoss: 1.1658, Accuracy: 72.58%\nLoss: 1.1652, Accuracy: 72.59%\nLoss: 1.1646, Accuracy: 72.59%\nLoss: 1.1640, Accuracy: 72.60%\nLoss: 1.1632, Accuracy: 72.62%\nLoss: 1.1626, Accuracy: 72.63%\nLoss: 1.1620, Accuracy: 72.64%\nLoss: 1.1613, Accuracy: 72.65%\nLoss: 1.1606, Accuracy: 72.67%\nLoss: 1.1600, Accuracy: 72.68%\nLoss: 1.1595, Accuracy: 72.69%\nLoss: 1.1589, Accuracy: 72.71%\nLoss: 1.1584, Accuracy: 72.72%\nLoss: 1.1579, Accuracy: 72.73%\nLoss: 1.1573, Accuracy: 72.74%\nLoss: 1.1567, Accuracy: 72.76%\nLoss: 1.1560, Accuracy: 72.77%\nLoss: 1.1553, Accuracy: 72.79%\nLoss: 1.1548, Accuracy: 72.80%\nLoss: 1.1542, Accuracy: 72.82%\nLoss: 1.1537, Accuracy: 72.83%\nLoss: 1.1533, Accuracy: 72.83%\nLoss: 1.1528, Accuracy: 72.85%\nLoss: 1.1520, Accuracy: 72.86%\nLoss: 1.1517, Accuracy: 72.88%\nLoss: 1.1513, Accuracy: 72.89%\nLoss: 1.1511, Accuracy: 72.89%\nLoss: 1.1506, Accuracy: 72.90%\nLoss: 1.1503, Accuracy: 72.91%\nLoss: 1.1500, Accuracy: 72.92%\nLoss: 1.1497, Accuracy: 72.93%\nLoss: 1.1493, Accuracy: 72.94%\nLoss: 1.1490, Accuracy: 72.95%\nLoss: 1.1487, Accuracy: 72.96%\nLoss: 1.1487, Accuracy: 72.95%\nLoss: 1.1485, Accuracy: 72.96%\nLoss: 1.1482, Accuracy: 72.96%\nLoss: 1.1479, Accuracy: 72.97%\nLoss: 1.1480, Accuracy: 72.97%\nLoss: 1.1479, Accuracy: 72.97%\nLoss: 1.1479, Accuracy: 72.98%\nLoss: 1.1477, Accuracy: 72.98%\nLoss: 1.1478, Accuracy: 72.99%\nLoss: 1.1476, Accuracy: 73.00%\nLoss: 1.1475, Accuracy: 73.00%\nLoss: 1.1474, Accuracy: 73.01%\nLoss: 1.1473, Accuracy: 73.01%\nLoss: 1.1472, Accuracy: 73.02%\nLoss: 1.1472, Accuracy: 73.02%\nLoss: 1.1472, Accuracy: 73.02%\nLoss: 1.1473, Accuracy: 73.02%\nLoss: 1.1474, Accuracy: 73.02%\nLoss: 1.1474, Accuracy: 73.03%\nLoss: 1.1474, Accuracy: 73.03%\nLoss: 1.1475, Accuracy: 73.02%\nLoss: 1.1476, Accuracy: 73.02%\nLoss: 1.1478, Accuracy: 73.01%\nLoss: 1.1480, Accuracy: 73.01%\nLoss: 1.1482, Accuracy: 73.01%\nLoss: 1.1485, Accuracy: 73.00%\nLoss: 1.1487, Accuracy: 73.00%\nLoss: 1.1490, Accuracy: 73.00%\nLoss: 1.1493, Accuracy: 72.99%\nLoss: 1.1497, Accuracy: 72.99%\nLoss: 1.1499, Accuracy: 72.98%\nLoss: 1.1502, Accuracy: 72.98%\nLoss: 1.1504, Accuracy: 72.97%\nLoss: 1.1506, Accuracy: 72.97%\nLoss: 1.1507, Accuracy: 72.96%\nLoss: 1.1510, Accuracy: 72.96%\nLoss: 1.1514, Accuracy: 72.95%\nLoss: 1.1517, Accuracy: 72.93%\nLoss: 1.1520, Accuracy: 72.92%\nLoss: 1.1524, Accuracy: 72.92%\nLoss: 1.1524, Accuracy: 72.91%\nLoss: 1.1525, Accuracy: 72.91%\nLoss: 1.1524, Accuracy: 72.92%\nLoss: 1.1525, Accuracy: 72.92%\nLoss: 1.1525, Accuracy: 72.91%\nLoss: 1.1525, Accuracy: 72.91%\nLoss: 1.1525, Accuracy: 72.91%\nLoss: 1.1524, Accuracy: 72.91%\nLoss: 1.1522, Accuracy: 72.91%\nLoss: 1.1522, Accuracy: 72.91%\nLoss: 1.1522, Accuracy: 72.90%\nLoss: 1.1518, Accuracy: 72.90%\nLoss: 1.1516, Accuracy: 72.90%\nExiting loop\n","output_type":"stream"}]},{"cell_type":"code","source":"poem_gen.generate_poem(num_poems=1, sample_size = 340, seed_word = \"Love is \")","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:41:41.530592Z","iopub.execute_input":"2023-08-23T06:41:41.531275Z","iopub.status.idle":"2023-08-23T06:41:41.646991Z","shell.execute_reply.started":"2023-08-23T06:41:41.531241Z","shell.execute_reply":"2023-08-23T06:41:41.645646Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"---------------\n; And An did see th an him cr pepthe Lupercal I thrice presented hin him apr hen he was ac presenthe Lupercal Did the Brutus in him hit on the presentel I that on the presentus his an him aa I thic pept: An the coural I thrice presend, hi Len he thon the Lupercal I thrcint cresentutu crienercal I thrice prneses him a kingly crind ghe pres\n","output_type":"stream"}]},{"cell_type":"code","source":"poem_gen.generate_poem(num_poems=1, sample_size = 340, seed_word = None)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T06:42:12.582655Z","iopub.execute_input":"2023-08-23T06:42:12.583064Z","iopub.status.idle":"2023-08-23T06:42:12.689584Z","shell.execute_reply.started":"2023-08-23T06:42:12.583031Z","shell.execute_reply":"2023-08-23T06:42:12.688095Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"---------------\n; And thic presened thic Yee I the Luperc crinof seeptus him a kingly crcend the Len an him al did the Lunercal I thrice presaercal I thrine Brutus le man. You al I thrice presed the Lupercal cris hima bitious; And thrae Luneral I thrice presented hion senty Call I thrin he Luperce preneral I thrice presentunercal I thrice presented hat o\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Next steps:\n\n* Evaluating the outputs:\n    Make a score of how \"good\" the outputed poem is. We will use the number of correct words first, and extend to see for grammatical correctness.\n* Fine-tuning the model: find better hyperparameters to maximize the score previously discussed.\n* Conditioning the output:\n    Scrape more poems, group them by categories( author, subject), and better condition the output based on the author and the subject.","metadata":{}}]}